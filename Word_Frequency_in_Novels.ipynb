{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx5jNBfRCEYhlxSfR94iNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalSharma2001/Data-Analysis-Projects/blob/main/Word_Frequency_in_Novels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text-Processing for Novel**\n",
        "In this analysis, we will look what are the most frequent words in **Herman Melville's Novel**, Moby Dick, and how often do they occur ?\n",
        "\n",
        "In this notebook, we'll scrape the novel *Moby Dick* from the website [Project Gutenberg](https://www.gutenberg.org/) (which contain a large corpus of books) using Python Packhage requests. Then we'll extract words from this web data using Beautiful Soup. Finally, we'll drive into analyzing the distribution of words using the nltk (Natural Language ToolKit) and Counter.\n",
        "\n",
        "The *Data Science pipeline* we'll build in this notebook can be used to visualise the word frequency at novel that you can find in the above link of Project Gutenberg. The NLP tools used here apply to much of the data that data scientist encounter as a vast proportion of the world's data is unstructured data and includes a great deal of texts.\n"
      ],
      "metadata": {
        "id": "fcjiL7ene92g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zKHvN_KOeJqX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "  import seaborn as sns\n",
        "except:\n",
        "  !pip install seaborn --user\n",
        "  import seaborn as sns"
      ],
      "metadata": {
        "id": "RRzs32JHhvnx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "1MCd5stHh_Fz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Moby Dick"
      ],
      "metadata": {
        "id": "Z4wraC7MiQXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n",
        "print(type(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKdbij6BiO02",
        "outputId": "0386b456-ef43-4ea5-e8e8-9f852f610c9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'requests.models.Response'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7iyq381jTG6",
        "outputId": "3b34b049-4366-423c-f41a-f7b8b4aac6d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the the output is **<Response [200]>** thus the data is successfully fetched."
      ],
      "metadata": {
        "id": "IpSAAUmbjWUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the text-encoding of the HTML Page\n",
        "data.encoding = 'utf-8'\n",
        "\n",
        "# Extracting the HTMKL from the request objects\n",
        "html = data.text"
      ],
      "metadata": {
        "id": "LHLf0SLtjU3Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an object using Beautiful from HTML\n",
        "sp = BeautifulSoup(html)\n",
        "\n",
        "# getting hyperlinks from soup and check out first several\n",
        "print(sp.findAll('a')[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoJz5X6gj0d5",
        "outputId": "11912cae-db4f-48fb-e997-c7416a28ef84"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<a href=\"#link2H_4_0002\"> ETYMOLOGY. </a>, <a href=\"#link2H_4_0003\"> EXTRACTS (Supplied by a Sub-Sub-Librarian).\r\n",
            "        </a>, <a href=\"#link2HCH0001\"> CHAPTER 1. Loomings. </a>, <a href=\"#link2HCH0002\"> CHAPTER 2. The Carpet-Bag. </a>, <a href=\"#link2HCH0003\"> CHAPTER 3. The Spouter-Inn. </a>, <a href=\"#link2HCH0004\"> CHAPTER 4. The Counterpane. </a>, <a href=\"#link2HCH0005\"> CHAPTER 5. Breakfast. </a>, <a href=\"#link2HCH0006\"> CHAPTER 6. The Street. </a>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us all the text under < a >"
      ],
      "metadata": {
        "id": "a2ugZO-LkPEr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8JdWkYrkL7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}