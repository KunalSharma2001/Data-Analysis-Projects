{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3jtK3tVmzSQYG5DN9cTYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunalSharma2001/Data-Analysis-Projects/blob/main/Word_Frequency_in_Novels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text-Processing for Novel**\n",
        "In this analysis, we will look what are the most frequent words in **Herman Melville's Novel**, Moby Dick, and how often do they occur ?\n",
        "\n",
        "In this notebook, we'll scrape the novel *Moby Dick* from the website [Project Gutenberg](https://www.gutenberg.org/) (which contain a large corpus of books) using Python Packhage requests. Then we'll extract words from this web data using Beautiful Soup. Finally, we'll drive into analyzing the distribution of words using the nltk (Natural Language ToolKit) and Counter.\n",
        "\n",
        "The *Data Science pipeline* we'll build in this notebook can be used to visualise the word frequency at novel that you can find in the above link of Project Gutenberg. The NLP tools used here apply to much of the data that data scientist encounter as a vast proportion of the world's data is unstructured data and includes a great deal of texts.\n"
      ],
      "metadata": {
        "id": "fcjiL7ene92g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zKHvN_KOeJqX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import nltk # extracting the words from the HTML\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "  import seaborn as sns\n",
        "except:\n",
        "  !pip install seaborn --user\n",
        "  import seaborn as sns"
      ],
      "metadata": {
        "id": "RRzs32JHhvnx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "1MCd5stHh_Fz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Moby Dick"
      ],
      "metadata": {
        "id": "Z4wraC7MiQXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n",
        "print(type(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKdbij6BiO02",
        "outputId": "df419f2a-ae4a-445b-974d-f599bc8b7bc1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'requests.models.Response'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7iyq381jTG6",
        "outputId": "481f566d-5234-46f1-eea0-8f80454eb87d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the the output is **<Response [200]>** thus the data is successfully fetched."
      ],
      "metadata": {
        "id": "IpSAAUmbjWUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the text-encoding of the HTML Page\n",
        "data.encoding = 'utf-8' \n",
        "\n",
        "# Extracting the HTMKL from the request objects\n",
        "html = data.text\n",
        "print(html[0:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHLf0SLtjU3Q",
        "outputId": "e1fb6c2c-7a7a-41ba-afe5-b83523d176e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n",
            "\r\n",
            "<!DOCTYPE html\r\n",
            "   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\r\n",
            "   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\r\n",
            "\r\n",
            "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\r\n",
            "  <head>\r\n",
            "    <title>\r\n",
            "      Moby Dick; Or the Whale, by Herman Melville\r\n",
            "    </title>\r\n",
            "    <style type=\"text/css\" xml:space=\"preserve\">\r\n",
            "\r\n",
            "    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\r\n",
            "    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\r\n",
            "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\r\n",
            "    hr  { width: 50%; text-align: center;}\r\n",
            "    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\r\n",
            "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\r\n",
            "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\r\n",
            "    .toc       { margin-left: 10%; margin-bottom: .75em;}\r\n",
            "    .toc2      { margin-left: 20%;}\r\n",
            "    div.fig    { display:block; margin:0 auto; text-align:center; }\r\n",
            "    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\r\n",
            "    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\r\n",
            "    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\r\n",
            "    .pagenum   {display:inline; font-size: 70%; font-style:normal;\r\n",
            "               margin: 0; padding: 0; position: absolute; right: 1%;\r\n",
            "               text-align: right;}\r\n",
            "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\r\n",
            "\r\n",
            "    table      {margin-left: 10%;}\r\n",
            "\r\n",
            "a:link {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "link {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "a:visited {color:blue;\r\n",
            "\t\ttext-decoration:none}\r\n",
            "a:hover {color:red}\r\n",
            "\r\n",
            "</style>\r\n",
            "  </head>\r\n",
            "  <body>\r\n",
            "<pre xml:space=\"preserve\">\r\n",
            "\r\n",
            "The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an object using Beautiful from HTML\n",
        "sp = BeautifulSoup(html, 'html.parser')\n",
        "text = sp.get_text()\n",
        "# text\n",
        "\n",
        "# Printing out text between characters 32000 and 34000\n",
        "print(text[32000:34000])\n",
        "# getting hyperlinks from soup and check out first several\n",
        "# print(sp.findAll('a')[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoJz5X6gj0d5",
        "outputId": "454a09e1-3b88-4cd9-fc16-f772d732261a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ent me\r\n",
            "      from deliberately stepping into the street, and methodically knocking\r\n",
            "      people’s hats off—then, I account it high time to get to sea as soon\r\n",
            "      as I can. This is my substitute for pistol and ball. With a philosophical\r\n",
            "      flourish Cato throws himself upon his sword; I quietly take to the ship.\r\n",
            "      There is nothing surprising in this. If they but knew it, almost all men\r\n",
            "      in their degree, some time or other, cherish very nearly the same feelings\r\n",
            "      towards the ocean with me.\r\n",
            "    \n",
            "\r\n",
            "      There now is your insular city of the Manhattoes, belted round by wharves\r\n",
            "      as Indian isles by coral reefs—commerce surrounds it with her surf.\r\n",
            "      Right and left, the streets take you waterward. Its extreme downtown is\r\n",
            "      the battery, where that noble mole is washed by waves, and cooled by\r\n",
            "      breezes, which a few hours previous were out of sight of land. Look at the\r\n",
            "      crowds of water-gazers there.\r\n",
            "    \n",
            "\r\n",
            "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\r\n",
            "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\r\n",
            "      you see?—Posted like silent sentinels all around the town, stand\r\n",
            "      thousands upon thousands of mortal men fixed in ocean reveries. Some\r\n",
            "      leaning against the spiles; some seated upon the pier-heads; some looking\r\n",
            "      over the bulwarks of ships from China; some high aloft in the rigging, as\r\n",
            "      if striving to get a still better seaward peep. But these are all\r\n",
            "      landsmen; of week days pent up in lath and plaster—tied to counters,\r\n",
            "      nailed to benches, clinched to desks. How then is this? Are the green\r\n",
            "      fields gone? What do they here?\r\n",
            "    \n",
            "\r\n",
            "      But look! here come more crowds, pacing straight for the water, and\r\n",
            "      seemingly bound for a dive. Strange! Nothing will content them but the\r\n",
            "      extremest limit of the land; loitering under the shady lee of yonder\r\n",
            "      warehouses will not suffice. No. They must get just as nigh th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us all the text under < a >"
      ],
      "metadata": {
        "id": "a2ugZO-LkPEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8JdWkYrkL7J",
        "outputId": "a401e763-3f01-428b-d75a-5e7792469b7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Moby',\n",
              " 'Dick',\n",
              " 'Or',\n",
              " 'the',\n",
              " 'Whale',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " 'The',\n",
              " 'Project']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the text of the novel! There is some unwanted stuff at the start and some unwanted stuff at the end. We could remove it, but this content is so much smaller in amount than the text of Moby Dick that, to a first approximation, it is okay to leave it in.\n",
        "\n",
        "Now that we have the text of interest, it's time to count how many times each word appears, and for this we'll use nltk – the Natural Language Toolkit. We'll start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) and then split the text into a list of words."
      ],
      "metadata": {
        "id": "NtKENFeln0a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making the words lower case\n",
        "words = [token.lower() for token in tokens]\n",
        "words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDauufzWngYz",
        "outputId": "20c3b086-8000-4e71-d014-2325f7d473eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['moby',\n",
              " 'dick',\n",
              " 'or',\n",
              " 'the',\n",
              " 'whale',\n",
              " 'by',\n",
              " 'herman',\n",
              " 'melville',\n",
              " 'the',\n",
              " 'project']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "# from nltk.corpus import stopwords\n",
        "# sw = stopwords.word('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdH0DpmMoJKp",
        "outputId": "726c03a1-537f-4e7b-9c03-a3e861d32f6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> stopwords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package stopwords to /root/nltk_data...\n",
            "      Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> stopwords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package stopwords to /root/nltk_data...\n",
            "      Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have to download the nltk.corpus stopwords. And the list of Stopwords is downloaded and we can import it and use it."
      ],
      "metadata": {
        "id": "v9a5BHm8uZ1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('english')\n",
        "sw[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpoOMHMVtIjo",
        "outputId": "714254f3-58d2-4d2a-fe0e-ddeb5cd414f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing the stop words from the Moby Dick"
      ],
      "metadata": {
        "id": "jUTMLXnpu3yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "not_words = [word for word in words if words not in sw]\n",
        "not_words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUVeVtZquN6b",
        "outputId": "c95a630a-bad6-45e1-d32a-47da70d5b5f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['moby',\n",
              " 'dick',\n",
              " 'or',\n",
              " 'the',\n",
              " 'whale',\n",
              " 'by',\n",
              " 'herman',\n",
              " 'melville',\n",
              " 'the',\n",
              " 'project']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the words that are not the stop words in the Moby Dick."
      ],
      "metadata": {
        "id": "J1Ldi9OGvR2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "ldIHUSA0rCwx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Count = Counter(not_words)\n",
        "top_ten = Count.most_common(10)\n",
        "max(top_ten)"
      ],
      "metadata": {
        "id": "YzUf1GihvKyH",
        "outputId": "07c53d5a-b2a3-4f8f-8ccc-434b867813cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('to', 4707)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The natural language processing skills we used in this notebook are also applicable to much of the data that Data Scientists encounter as the vast proportion of the world's data is unstructured data and includes a great deal of text.\n",
        "\n",
        "So, what word turned out to (not surprisingly) be the most common word in Moby Dick?"
      ],
      "metadata": {
        "id": "FSfXwVvfsVJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nr-kgst6sVHq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKSnaa-0qcK0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}